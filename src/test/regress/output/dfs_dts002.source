set enable_global_stats = true;
--SQLONHADOOP-23
--test rumtime predicate for dfs
create schema dfs;
set current_schema = dfs;
drop table if exists sales;
NOTICE:  table "sales" does not exist, skipping
create   table sales(s_id int, b int1, c int2, d int8, e float4, f float8, option1 varchar(10), option2 clob, option3 bpchar(10), time2 timestamp,  customer_name text,expense int,item text) tablespace hdfs_ts;
set cstore_insert_mode = main;
insert into sales values(1, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54','zhangsan', 20 , 'book'     );
insert into sales values(2, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54','ngyi', 100, 'photo'    );
insert into sales values(3, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'lun', 43 , 'sigurates');
insert into sales values(4, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'ojian', 25 , 'food'     );
drop table if exists customer;
NOTICE:  table "customer" does not exist, skipping
create   table customer(id int, b int1, c int2, d int8, e float4, f float8, option1 varchar(10), option2 clob, option3 bpchar(10), time2 timestamp, customer_name text,age int,job text) tablespace hdfs_ts;
insert into customer values(1, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'zhangsan', 20, 'teacher');
insert into customer values(2, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'sangyi', 32, 'manong' )  ;
insert into customer values(3, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'ailun', 15, 'police' )   ;
insert into customer values(4, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'haojian', 57, 'chef'   ) ;
insert into customer values(6, 2, 3, 4, 5, 6, 'option1', 'option2', 'options3', '2004-10-19 10:23:54', 'haojian', 57, 'chef'   ) ;
explain (verbose on, costs off) select * from sales where customer_name =(select customer_name from customer where job='teacher');
                                                                                                                                              QUERY PLAN                                                                                                                                              
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Row Adapter
   Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
   ->  Vector Streaming (type: GATHER)
         Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 Output: dfs.customer.customer_name
                 ->  Vector Streaming(type: BROADCAST)
                       Output: dfs.customer.customer_name
                       ->  Vector Result
                             Output: dfs.customer.customer_name
                             ->  Vector Append
                                   ->  Dfs Scan on dfs.customer
                                         Output: dfs.customer.customer_name
                                         Distribute Key: dfs.customer.id
                                         Pushdown Predicate Filter: (dfs.customer.job = 'teacher'::text)
                                   ->  Vector Adapter
                                         Output: cstore.customer.customer_name
                                         ->  Seq Scan on cstore.pg_delta_dfs_customer customer
                                               Output: cstore.customer.customer_name
                                               Distribute Key: cstore.customer.id
                                               Filter: (cstore.customer.job = 'teacher'::text)
         ->  Vector Result
               Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
               ->  Vector Append
                     ->  Dfs Scan on dfs.sales
                           Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
                           Distribute Key: dfs.sales.s_id
                           Pushdown Predicate Filter: (dfs.sales.customer_name = $0)
                     ->  Vector Adapter
                           Output: cstore.sales.s_id, cstore.sales.b, cstore.sales.c, cstore.sales.d, cstore.sales.e, cstore.sales.f, cstore.sales.option1, cstore.sales.option2, cstore.sales.option3, cstore.sales.time2, cstore.sales.customer_name, cstore.sales.expense, cstore.sales.item
                           ->  Seq Scan on cstore.pg_delta_dfs_sales sales
                                 Output: cstore.sales.s_id, cstore.sales.b, cstore.sales.c, cstore.sales.d, cstore.sales.e, cstore.sales.f, cstore.sales.option1, cstore.sales.option2, cstore.sales.option3, cstore.sales.time2, cstore.sales.customer_name, cstore.sales.expense, cstore.sales.item
                                 Distribute Key: cstore.sales.s_id
                                 Filter: (cstore.sales.customer_name = $0)
(35 rows)

select * from sales where customer_name =(select customer_name from customer where job='teacher');
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

explain (verbose on, costs off) select * from sales where s_id =(select id from customer where id=1);
                                                                                                                                              QUERY PLAN                                                                                                                                              
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Row Adapter
   Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
   ->  Vector Streaming (type: GATHER)
         Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 Output: dfs.customer.id
                 ->  Vector Streaming(type: BROADCAST)
                       Output: dfs.customer.id
                       ->  Vector Result
                             Output: dfs.customer.id
                             ->  Vector Append
                                   ->  Dfs Scan on dfs.customer
                                         Output: dfs.customer.id
                                         Distribute Key: dfs.customer.id
                                         Pushdown Predicate Filter: (dfs.customer.id = 1)
                                   ->  Vector Adapter
                                         Output: cstore.customer.id
                                         ->  Seq Scan on cstore.pg_delta_dfs_customer customer
                                               Output: cstore.customer.id
                                               Distribute Key: cstore.customer.id
                                               Filter: (cstore.customer.id = 1)
         ->  Vector Result
               Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
               ->  Vector Append
                     ->  Dfs Scan on dfs.sales
                           Output: dfs.sales.s_id, dfs.sales.b, dfs.sales.c, dfs.sales.d, dfs.sales.e, dfs.sales.f, dfs.sales.option1, dfs.sales.option2, dfs.sales.option3, dfs.sales.time2, dfs.sales.customer_name, dfs.sales.expense, dfs.sales.item
                           Distribute Key: dfs.sales.s_id
                           Pushdown Predicate Filter: (dfs.sales.s_id = $0)
                     ->  Vector Adapter
                           Output: cstore.sales.s_id, cstore.sales.b, cstore.sales.c, cstore.sales.d, cstore.sales.e, cstore.sales.f, cstore.sales.option1, cstore.sales.option2, cstore.sales.option3, cstore.sales.time2, cstore.sales.customer_name, cstore.sales.expense, cstore.sales.item
                           ->  Seq Scan on cstore.pg_delta_dfs_sales sales
                                 Output: cstore.sales.s_id, cstore.sales.b, cstore.sales.c, cstore.sales.d, cstore.sales.e, cstore.sales.f, cstore.sales.option1, cstore.sales.option2, cstore.sales.option3, cstore.sales.time2, cstore.sales.customer_name, cstore.sales.expense, cstore.sales.item
                                 Distribute Key: cstore.sales.s_id
                                 Filter: (cstore.sales.s_id = $0)
(35 rows)

select * from sales where s_id =(select id from customer where id=1);
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

--add llt
select * from sales where b =(select b from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where c =(select c from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where d =(select d from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where e =(select e from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where f =(select f from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where option1 =(select option1 from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where option2 =(select option2 from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where option3 =(select option3 from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

select * from sales where time2 =(select time2 from customer where job='teacher') order by 1, 2, 3 limit 1;
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

create   table test_delta_table(id int) tablespace hdfs_ts;
CREATE OR REPLACE FUNCTION delta_table()
RETURNS TABLE (rowcount bigint) AS
$$
declare tblname text;
BEGIN
select 'select count(*) from cstore.pg_dfsdesc_'||a.oid into tblname from pg_class a where a.relname='test_delta_table';

RETURN QUERY execute tblname;
END;
$$ LANGUAGE plpgsql;
select delta_table();
--?select count\(\*\) from cstore.pg_dfsdesc_.*
--?ERROR:  The relation "pg_dfsdesc_.*" has no distribute type.
--?.*
referenced column: delta_table
drop table test_delta_table;
--test rumtime predicate on hdfs foreign table
CREATE SERVER hdfs_server FOREIGN DATA WRAPPER 	HDFS_FDW OPTIONS (type 'hdfs', address '@hdfshostname@:@hdfsport@',hdfscfgpath '@hdfscfgpath@');
create schema hdfs;
set current_schema=hdfs;
create foreign table sales(s_id int, b int1, c int2, d int8, e float4, f float8, option1 varchar(10), option2 clob, option3 bpchar(10), time2 timestamp,customer_name text,expense int,item text)server hdfs_server options(format 'orc',foldername '/@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/dfs.sales')distribute by roundrobin;
create foreign table customer(id int, b int1, c int2, d int8, e float4, f float8, option1 varchar(10), option2 clob, option3 bpchar(10), time2 timestamp,customer_name text,age int,job text)server hdfs_server options(format 'orc',foldername '/@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/dfs.customer')distribute by roundrobin;
analyze sales;
analyze customer;
explain (verbose on, costs off) select * from sales where customer_name =(select customer_name from customer where job='teacher');
--?.*
--?.*
 Row Adapter
   Output: sales.s_id, sales.b, sales.c, sales.d, sales.e, sales.f, sales.option1, sales.option2, sales.option3, sales.time2, sales.customer_name, sales.expense, sales.item
   ->  Vector Streaming (type: GATHER)
         Output: sales.s_id, sales.b, sales.c, sales.d, sales.e, sales.f, sales.option1, sales.option2, sales.option3, sales.time2, sales.customer_name, sales.expense, sales.item
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 Output: customer.customer_name
                 ->  Vector Streaming(type: BROADCAST)
                       Output: customer.customer_name
                       ->  Vector Foreign Scan on hdfs.customer
                             Output: customer.customer_name
                             Pushdown Predicate Filter: (customer.job = 'teacher'::text)
                             Server Type: hdfs
--?.*
         ->  Vector Foreign Scan on hdfs.sales
               Output: sales.s_id, sales.b, sales.c, sales.d, sales.e, sales.f, sales.option1, sales.option2, sales.option3, sales.time2, sales.customer_name, sales.expense, sales.item
               Pushdown Predicate Filter: (sales.customer_name = $0)
               Server Type: hdfs
--?.*
(19 rows)

select * from sales where customer_name =(select customer_name from customer where job='teacher');
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

explain (verbose on, costs off) select * from sales where s_id =(select id from customer where id=1);
--?.*     
--?.*
 Row Adapter
   Output: sales.s_id, sales.b, sales.c, sales.d, sales.e, sales.f, sales.option1, sales.option2, sales.option3, sales.time2, sales.customer_name, sales.expense, sales.item
   ->  Vector Streaming (type: GATHER)
         Output: sales.s_id, sales.b, sales.c, sales.d, sales.e, sales.f, sales.option1, sales.option2, sales.option3, sales.time2, sales.customer_name, sales.expense, sales.item
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 Output: customer.id
                 ->  Vector Streaming(type: BROADCAST)
                       Output: customer.id
                       ->  Vector Foreign Scan on hdfs.customer
                             Output: customer.id
                             Pushdown Predicate Filter: (customer.id = 1)
                             Server Type: hdfs
--?.*
         ->  Vector Foreign Scan on hdfs.sales
               Output: sales.s_id, sales.b, sales.c, sales.d, sales.e, sales.f, sales.option1, sales.option2, sales.option3, sales.time2, sales.customer_name, sales.expense, sales.item
               Pushdown Predicate Filter: (sales.s_id = $0)
               Server Type: hdfs
--?.*
(19 rows)

select * from sales where s_id =(select id from customer where id=1);
 s_id | b | c | d | e | f | option1 | option2 |  option3   |          time2           | customer_name | expense | item 
------+---+---+---+---+---+---------+---------+------------+--------------------------+---------------+---------+------
    1 | 2 | 3 | 4 | 5 | 6 | option1 | option2 | options3   | Tue Oct 19 10:23:54 2004 | zhangsan      |      20 | book
(1 row)

drop foreign table if exists sales;
drop foreign table if exists customer;
set current_schema = dfs;
drop table col_subplan;
ERROR:  table "col_subplan" does not exist
create table col_subplan(a int, b char(2)) with(orientation=column) distribute by replication;
insert into col_subplan values(1,'12');
insert into col_subplan values(2,NULL);
create table dfs_subplan(a int, b char(2)) tablespace hdfs_ts;
set cstore_insert_mode='main';
insert into dfs_subplan values(3,NULL);
insert into dfs_subplan values(3,'45');
insert into dfs_subplan values(3,'12');
select * from dfs_subplan where b = (select b from col_subplan where a = 1);
 a | b  
---+----
 3 | 12
(1 row)

select * from dfs_subplan where b = (select b from col_subplan where a = 2);
 a | b 
---+---
(0 rows)

drop table dfs_subplan;
drop table col_subplan;
reset cstore_insert_mode;
drop table if exists dts_002_warehouse;
NOTICE:  table "dts_002_warehouse" does not exist, skipping
create table dts_002_warehouse ( w_warehouse_sk integer not null, w_warehouse_id char(16) not null, w_warehouse_name varchar(20) , w_warehouse_sq_ft integer , w_street_number char(10) , w_street_name varchar(60) , w_street_type char(15) , w_suite_number char(10) , w_city varchar(60) , w_county varchar(30) , w_state char(2) , w_zip char(10) , w_country varchar(20) , w_gmt_offset decimal(5,2) )tablespace hdfs_ts;
explain (verbose on, costs off) select  max(abs((select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc')))  from dts_002_warehouse order by (select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc');
                                                         QUERY PLAN                                                          
-----------------------------------------------------------------------------------------------------------------------------
 Result
   Output: $2, $1
   InitPlan 1 (returns $0)
     ->  Row Adapter
           Output: dfs.dts_002_warehouse.w_gmt_offset
           ->  Vector Streaming(type: BROADCAST)
                 Output: dfs.dts_002_warehouse.w_gmt_offset
                 ->  Vector Result
                       Output: dfs.dts_002_warehouse.w_gmt_offset
                       ->  Vector Append
                             ->  Dfs Scan on dfs.dts_002_warehouse
                                   Output: dfs.dts_002_warehouse.w_gmt_offset
                                   Distribute Key: dfs.dts_002_warehouse.w_warehouse_sk
                                   Pushdown Predicate Filter: ((dfs.dts_002_warehouse.w_warehouse_name)::text = 'cc'::text)
                             ->  Vector Adapter
                                   Output: cstore.dts_002_warehouse.w_gmt_offset
                                   ->  Seq Scan on cstore.pg_delta_dfs_dts_002_warehouse dts_002_warehouse
                                         Output: cstore.dts_002_warehouse.w_gmt_offset
                                         Distribute Key: cstore.dts_002_warehouse.w_warehouse_sk
                                         Filter: ((cstore.dts_002_warehouse.w_warehouse_name)::text = 'cc'::text)
   InitPlan 2 (returns $1)
     ->  Row Adapter
           Output: dfs.dts_002_warehouse.w_gmt_offset
           ->  Vector Streaming (type: GATHER)
                 Output: dfs.dts_002_warehouse.w_gmt_offset
                 ->  Vector Result
                       Output: dfs.dts_002_warehouse.w_gmt_offset
                       ->  Vector Append
                             ->  Dfs Scan on dfs.dts_002_warehouse
                                   Output: dfs.dts_002_warehouse.w_gmt_offset
                                   Distribute Key: dfs.dts_002_warehouse.w_warehouse_sk
                                   Pushdown Predicate Filter: ((dfs.dts_002_warehouse.w_warehouse_name)::text = 'cc'::text)
                             ->  Vector Adapter
                                   Output: cstore.dts_002_warehouse.w_gmt_offset
                                   ->  Seq Scan on cstore.pg_delta_dfs_dts_002_warehouse dts_002_warehouse
                                         Output: cstore.dts_002_warehouse.w_gmt_offset
                                         Distribute Key: cstore.dts_002_warehouse.w_warehouse_sk
                                         Filter: ((cstore.dts_002_warehouse.w_warehouse_name)::text = 'cc'::text)
   InitPlan 3 (returns $2)
     ->  Row Adapter
           Output: (max((abs($0))))
           ->  Vector Aggregate
                 Output: max((abs($0)))
                 ->  Vector Streaming (type: GATHER)
                       Output: (abs($0))
                       ->  Vector Limit
                             Output: (abs($0))
                             ->  Vector Result
                                   Output: abs($0)
                                   One-Time Filter: (abs($0) IS NOT NULL)
                                   ->  Vector Append
                                         ->  Vector Result
                                               Output: ('Dummy')
                                               One-Time Filter: (abs($0) IS NOT NULL)
                                               ->  Dfs Scan on dfs.dts_002_warehouse
                                                     Output: 'Dummy'
                                                     Distribute Key: dfs.dts_002_warehouse.w_warehouse_sk
                                         ->  Vector Result
                                               Output: ('Dummy')
                                               One-Time Filter: (abs($0) IS NOT NULL)
                                               ->  Vector Adapter
                                                     Output: ('Dummy')
                                                     ->  Seq Scan on cstore.pg_delta_dfs_dts_002_warehouse dts_002_warehouse
                                                           Output: 'Dummy'
                                                           Distribute Key: cstore.dts_002_warehouse.w_warehouse_sk
(65 rows)

set cstore_insert_mode = main;
insert into dts_002_warehouse values(1, 'bb', 'cc', 4, '55', '66', '77', '88', '99', '1010', '11', '12', '13', 14.1);
select  max(abs((select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc')))  from dts_002_warehouse order by (select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc');
  max  
-------
 14.10
(1 row)

insert into dts_002_warehouse values(1, 'bb', 'cc', 4, '55', '66', '77', '88', '99', '1010', '11', '12', '13', 14.1);
-- throw error
select  max(abs((select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc')))  from dts_002_warehouse order by (select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc');
ERROR:  more than one row returned by a subquery used as an expression
CONTEXT:  referenced column: max
delete from dts_002_warehouse;
insert into dts_002_warehouse values(1, 'bb', 'cc', 4, '55', '66', '77', '88', '99', '1010', '11', '12', '13', 14.1);
select  max(abs((select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc')))  from dts_002_warehouse order by (select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc');
  max  
-------
 14.10
(1 row)

set cstore_insert_mode = delta;
insert into dts_002_warehouse values(1, 'bb', 'cc', 4, '55', '66', '77', '88', '99', '1010', '11', '12', '13', 14.1);
select  max(abs((select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc')))  from dts_002_warehouse order by (select w_gmt_offset  from dts_002_warehouse where w_warehouse_name = 'cc');
ERROR:  more than one row returned by a subquery used as an expression
CONTEXT:  referenced column: max
explain (verbose on, costs off) select max(abs(5.00)) from dts_002_warehouse;
                                                      QUERY PLAN                                                       
-----------------------------------------------------------------------------------------------------------------------
 Result
   Output: $0
   InitPlan 1 (returns $0)
     ->  Row Adapter
           Output: (max((5.00)))
           ->  Vector Aggregate
                 Output: max((5.00))
                 ->  Vector Streaming (type: GATHER)
                       Output: (5.00)
                       ->  Vector Limit
                             Output: (5.00)
                             ->  Vector Result
                                   Output: 5.00
                                   One-Time Filter: (5.00 IS NOT NULL)
                                   ->  Vector Append
                                         ->  Dfs Scan on dfs.dts_002_warehouse
                                               Output: 'Dummy'
                                               Distribute Key: dfs.dts_002_warehouse.w_warehouse_sk
                                         ->  Vector Adapter
                                               Output: ('Dummy')
                                               ->  Seq Scan on cstore.pg_delta_dfs_dts_002_warehouse dts_002_warehouse
                                                     Output: 'Dummy'
                                                     Distribute Key: cstore.dts_002_warehouse.w_warehouse_sk
(23 rows)

select max(abs(5.00)) from dts_002_warehouse;
 max  
------
 5.00
(1 row)

drop table dts_002_warehouse;
drop table if exists dfs_3;
NOTICE:  table "dfs_3" does not exist, skipping
drop table if exists dfs_4;
NOTICE:  table "dfs_4" does not exist, skipping
create table dfs_3( a int, b decimal(7,2)) tablespace hdfs_ts;
create table dfs_4( a int, b decimal(7,2)) tablespace hdfs_ts;
set cstore_insert_mode=main;
insert into dfs_3 values(12, 3.4);
insert into dfs_4 values(12, 3.4);
explain (verbose on, costs off) select * from (select 
cast(0 as decimal(7,2)) as return_amt
from dfs_3
union all
select 
b as return_amt
from dfs_3) order by 1 limit 5;
                                         QUERY PLAN                                          
---------------------------------------------------------------------------------------------
 Row Adapter
   Output: (0.00::numeric(7,2))
   ->  Vector Limit
         Output: (0.00::numeric(7,2))
         ->  Vector Streaming (type: GATHER)
               Output: (0.00::numeric(7,2))
               Merge Sort Key: (0.00::numeric(7,2))
               ->  Vector Limit
                     Output: (0.00::numeric(7,2))
                     ->  Vector Sort
                           Output: (0.00::numeric(7,2))
                           Sort Key: (0.00::numeric(7,2))
                           ->  Vector Result
                                 Output: (0.00::numeric(7,2))
                                 ->  Vector Append
                                       ->  Dfs Scan on dfs.dfs_3
                                             Output: 0.00::numeric(7,2)
                                             Distribute Key: dfs.dfs_3.a
                                       ->  Vector Adapter
                                             Output: (0.00::numeric(7,2))
                                             ->  Seq Scan on cstore.pg_delta_dfs_dfs_3 dfs_3
                                                   Output: 0.00::numeric(7,2)
                                                   Distribute Key: cstore.dfs_3.a
                                       ->  Dfs Scan on dfs.dfs_3
                                             Output: dfs.dfs_3.b
                                             Distribute Key: dfs.dfs_3.a
                                       ->  Vector Adapter
                                             Output: cstore.dfs_3.b
                                             ->  Seq Scan on cstore.pg_delta_dfs_dfs_3 dfs_3
                                                   Output: cstore.dfs_3.b
                                                   Distribute Key: cstore.dfs_3.a
(31 rows)

select * from (select 
cast(0 as decimal(7,2)) as return_amt
from dfs_3
union all
select 
b as return_amt
from dfs_3) order by 1 limit 5;
 return_amt 
------------
       0.00
       3.40
(2 rows)

drop table if exists dfs_3;
drop table if exists dfs_4;
create table dfs_dts002_dfs_001(a int, b int) tablespace hdfs_ts;
\dt+ dfs_dts002_dfs_001
--?.*
--?.*
--?.*
--? dfs    | dfs_dts002_dfs_001 | table |.*| 192 kB | 
(1 row)

insert into dfs_dts002_dfs_001 values(12, 13);
\dt+ dfs_dts002_dfs_001
--?.*
--?.*
--?.*
--? dfs    | dfs_dts002_dfs_001 | table |.*| 208 kB | 
(1 row)

set cstore_insert_mode=main;
insert into dfs_dts002_dfs_001 values(generate_series(1,100), generate_series(1,100));
\dt+ dfs_dts002_dfs_001
--?.*
--?.*
--?.*
--? dfs    | dfs_dts002_dfs_001 | table |.*| 409 kB | 
(1 row)

\! @abs_bindir@/gsql -d regression -p @dn1port@ -c '\dt+ dfs.dfs_dts002_dfs_001'
--?.*
--?.*
--?.*
--? dfs    | dfs_dts002_dfs_001 | table |.*
(1 row)

drop table dfs_dts002_dfs_001;
drop table if exists sales;
drop table if exists customer;
\! rm -rf @abs_srcdir@/tmp_check/dfs_init_004/hdfs_ts_test_size
drop tablespace if exists hdfs_ts_test_size;
NOTICE:  Tablespace "hdfs_ts_test_size" does not exist, skipping.
create  tablespace hdfs_ts_test_size location '@abs_srcdir@/tmp_check/dfs_init_004/hdfs_ts_test_size' with(filesystem='hdfs', address='@hdfshostname@:@hdfsport@', cfgpath='@hdfscfgpath@',storepath='/@hdfsstoreplus@/hdfs_ts_test_size');
  
create database test_size_db1;
\c test_size_db1
SELECT PG_SIZE_pretty(pg_database_size('test_size_db1'));
 pg_size_pretty 
----------------
--?.*
(1 row)

SELECT PG_SIZE_pretty(pg_tablespace_size('hdfs_ts_test_size'));
 pg_size_pretty 
----------------
--?.*
(1 row)

drop table if exists row_001;
NOTICE:  table "row_001" does not exist, skipping
create table row_001(a int, b int);
insert into row_001 values (generate_series(0, 1000), generate_series(0, 1000));
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
insert into row_001 select * from row_001;
\dt+ row_001
--?.*
--?.*
--?.*
--?.*19 MB | 
(1 row)

SELECT PG_SIZE_pretty(pg_database_size('test_size_db1'));
 pg_size_pretty 
----------------
--?.*
(1 row)

SELECT PG_SIZE_pretty(pg_tablespace_size('hdfs_ts_test_size'));	
 pg_size_pretty 
----------------
--?.*
(1 row)

 set cstore_insert_mode=main;
 drop table if exists dfs_test_size;
NOTICE:  table "dfs_test_size" does not exist, skipping
 create table dfs_test_size(a int, b int) tablespace hdfs_ts_test_size;
 insert into dfs_test_size select * from row_001;
 insert into dfs_test_size select * from dfs_test_size;
 insert into dfs_test_size select * from dfs_test_size;
 insert into dfs_test_size select * from dfs_test_size;
 \dt+ dfs_test_size
--?.*
--?.*
--?.*
--?.*11 MB | 
(1 row)

SELECT PG_SIZE_pretty(pg_database_size('test_size_db1'));
 pg_size_pretty 
----------------
--?.*
(1 row)

SELECT PG_SIZE_pretty(pg_tablespace_size('hdfs_ts_test_size'));	
 pg_size_pretty 
----------------
--?.*
(1 row)

alter tablespace hdfs_ts_test_size resize maxsize '1M';
insert into dfs_test_size select * from dfs_test_size;
set cstore_insert_mode = delta;
insert into dfs_test_size select * from row_001;
\c regression
drop database test_size_db1;
drop tablespace hdfs_ts_test_size;
--test partition DFS table
\! rm -rf @abs_srcdir@/tmp_check/hdfs_ts_part
drop tablespace if exists hdfs_ts_part;
NOTICE:  Tablespace "hdfs_ts_part" does not exist, skipping.
create  tablespace hdfs_ts_part location '@abs_srcdir@/tmp_check/hdfs_ts_part' with(filesystem='hdfs', address='@hdfshostname@:@hdfsport@', cfgpath='@hdfscfgpath@',storepath='/@hdfsstoreplus@/hdfs_ts_part');
create table dfs_dts_001_p( a int, b int) tablespace hdfs_ts_part partition by values(b);
set cstore_insert_mode=main;
insert into dfs_dts_001_p values(12,12);
drop table dfs_dts_001_p;
drop table if exists demographics cascade;
NOTICE:  table "demographics" does not exist, skipping
create table demographics
(
    zipcode char(5) not null ,
    populationsf3 double precision null 
)tablespace hdfs_ts_part distribute by hash(zipcode);
set cstore_insert_mode=main;
insert into demographics values ('c', null);
insert into demographics values ('d', 3.2 );
select (select dem_1.populationsf3  
from demographics dem_1  
order by dem_1.populationsf3 
desc limit 1) col1,dem.populationsf3  
from demographics dem 
where dem.populationsf3>col1;
 col1 | populationsf3 
------+---------------
(0 rows)

drop table if exists demographics cascade;
--
drop table HDFS_ADD_DROP_COLUMN_TABLE_022;
ERROR:  table "hdfs_add_drop_column_table_022" does not exist
CREATE TABLE HDFS_ADD_DROP_COLUMN_TABLE_022
(
sn int,
c_tinyint tinyint,
c_smallint smallint,
c_int integer,
c_bigint bigint,
c_numeric numeric,
c_decimal decimal,
c_real real,
c_float4 float4,
c_float8 float8,
c_double1 double precision,
c_double2 binary_double,
c_char1 char(100),
c_char2 character(100),
c_char3 nchar(100),
c_char4 varchar(100),
c_char5 character varying(100),
c_char6 nvarchar2(100),
c_char7 varchar2(100),
c_clob clob,
c_text text,
c_date date,
c_time1 time with time zone,
c_time2 time without time zone,
c_timestamp1 timestamp with time zone,
c_timestamp2 timestamp without time zone,
c_smalldatetime smalldatetime,
c_interval interval,
c_boolean boolean,
c_oid oid,
c_money money
)
WITH(orientation = 'orc',version = 0.12) TABLESPACE hdfs_ts DISTRIBUTE BY HASH(sn);
CREATE INDEX index_drop_022 ON HDFS_ADD_DROP_COLUMN_TABLE_022(sn,c_tinyint);
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_tinyint;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_smallint;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_int;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_bigint;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_numeric;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_decimal;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_real;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_float4;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_float8;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_double1;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_double2;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char1;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char2;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char3;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char4;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char5;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char6;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_char7;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_clob;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_text;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_date;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_time1;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_time2;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_timestamp1;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_timestamp2;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_smalldatetime;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_interval;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_boolean;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_oid;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 DROP COLUMN c_money;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 ADD COLUMN c_char1 char(100) null;
ALTER TABLE HDFS_ADD_DROP_COLUMN_TABLE_022 ADD COLUMN c_date date null;
SET cstore_insert_mode=auto;
COPY HDFS_ADD_DROP_COLUMN_TABLE_022 FROM '@abs_srcdir@/data/HDFS_ADD_DROP_COLUMN_TABLE_022.txt' DELIMITER '|';
drop table HDFS_ADD_DROP_COLUMN_TABLE_022;
--
drop table if exists dfs_vacuum;
NOTICE:  table "dfs_vacuum" does not exist, skipping
create table dfs_vacuum ( a int, b int, c int) tablespace hdfs_ts;
set cstore_insert_mode=main;
insert into dfs_vacuum values(1, 2, 3);
set cstore_insert_mode=delta;
insert into dfs_vacuum values(1, 2, 3);
alter table dfs_vacuum drop column b;
vacuum full dfs_vacuum;
vacuum deltamerge dfs_vacuum;
select * from dfs_vacuum;
 a | c 
---+---
 1 | 3
 1 | 3
(2 rows)

drop table dfs_vacuum;
drop table if exists dfs_partial_tbl cascade;
NOTICE:  table "dfs_partial_tbl" does not exist, skipping
create table dfs_partial_tbl( a int, b  int, c int ) tablespace hdfs_ts; 
alter table dfs_partial_tbl add partial cluster key ( c, b);
\d+ dfs_partial_tbl
                   Table "public.dfs_partial_tbl"
 Column |  Type   | Modifiers | Storage | Stats target | Description 
--------+---------+-----------+---------+--------------+-------------
 a      | integer |           | plain   |              | 
 b      | integer |           | plain   |              | 
 c      | integer |           | plain   |              | 
Partial Cluster :
    "dfs_partial_tbl_cluster" PARTIAL CLUSTER KEY (c, b)
Has OIDs: no
Tablespace: "hdfs_ts"
Distribute By: HASH(a)
Location Nodes: ALL DATANODES
Options: orientation=orc, compression=snappy, version=0.12

alter table dfs_partial_tbl drop column b;
\d+ dfs_partial_tbl
                   Table "public.dfs_partial_tbl"
 Column |  Type   | Modifiers | Storage | Stats target | Description 
--------+---------+-----------+---------+--------------+-------------
 a      | integer |           | plain   |              | 
 c      | integer |           | plain   |              | 
Has OIDs: no
Tablespace: "hdfs_ts"
Distribute By: HASH(a)
Location Nodes: ALL DATANODES
Options: orientation=orc, compression=snappy, version=0.12

alter table dfs_partial_tbl add partial cluster key ( a );
\d+ dfs_partial_tbl
                   Table "public.dfs_partial_tbl"
 Column |  Type   | Modifiers | Storage | Stats target | Description 
--------+---------+-----------+---------+--------------+-------------
 a      | integer |           | plain   |              | 
 c      | integer |           | plain   |              | 
Partial Cluster :
    "dfs_partial_tbl_cluster" PARTIAL CLUSTER KEY (a)
Has OIDs: no
Tablespace: "hdfs_ts"
Distribute By: HASH(a)
Location Nodes: ALL DATANODES
Options: orientation=orc, compression=snappy, version=0.12

drop table if exists dfs_partial_tbl cascade;
show server_encoding;
 server_encoding 
-----------------
 UTF8
(1 row)

show client_encoding;
 client_encoding 
-----------------
 UTF8
(1 row)

drop table if exists customer_demographics_dts;
NOTICE:  table "customer_demographics_dts" does not exist, skipping
create table customer_demographics_dts
(
    cd_demo_sk                integer               not null,
    cd_gender                 char(1)           default 'a'  ,
    cd_marital_status         char(1)                       ,
    cd_education_status       char(20) default 'ajdosd' ,
    cd_purchase_estimate      integer                       ,
    cd_credit_rating          char(10)                      ,
    cd_dep_count              integer                       ,
    cd_dep_employed_count     integer          default '' ,
    cd_dep_college_count      integer                      
)  with(compression='no',orientation=orc,version=0.12) tablespace hdfs_ts
distribute by hash(cd_demo_sk);
set cstore_insert_mode=main;
--insert into customer_demographics_dts values(1,'M','M','Primary',500,'Good',0,0,0);
copy customer_demographics_dts from '@abs_srcdir@/data/customer_demographics_2.txt' DELIMITER as ',' NULL as '' ; 
select distinct cd_gender from customer_demographics_dts where  cd_gender ='M';
 cd_gender 
-----------
 M
(1 row)

select distinct cd_gender from customer_demographics_dts where  cd_gender <'M' order by 1;
 cd_gender 
-----------
 a
 F
(2 rows)

drop table if exists customer_demographics_dts;
drop table if exists dfs_001;
NOTICE:  table "dfs_001" does not exist, skipping
create table dfs_001( a int, b int, c int) tablespace hdfs_ts partition by values(b, c);
set cstore_insert_mode=main;
insert into dfs_001 values(1, 2);
insert into dfs_001 values(1, 3);  
  
drop foreign table if exists f_001;
NOTICE:  foreign table "f_001" does not exist, skipping
create foreign table f_001( a int, b int, c int)
server hdfs_server
OPTIONS (format 'orc', foldername '/@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.dfs_001')
distribute by replication
partition by (b, c) AUTOMAPPED;
explain (verbose, costs off) select * from f_001 where a=2;
--?.*
HINT:  Do analyze for them in order to generate optimized plan.
--?.*
--?.*
 Row Adapter
   Output: a, b, c
   ->  Vector Streaming (type: GATHER)
         Output: a, b, c
         ->  Partitioned Vector Foreign Scan on public.f_001
               Output: a, b, c
               Pushdown Predicate Filter: (f_001.a = 2)
               Server Type: hdfs
               Orc File: /@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.dfs_001
(9 rows)

explain (verbose, costs off) select * from f_001 where b=2;
--?.*
HINT:  Do analyze for them in order to generate optimized plan.
--?.*
--?.*
 Row Adapter
   Output: a, b, c
   ->  Vector Streaming (type: GATHER)
         Output: a, b, c
         ->  Partitioned Vector Foreign Scan on public.f_001
               Output: a, b, c
               Filter: (f_001.b = 2)
               Server Type: hdfs
               Orc File: /@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.dfs_001
               Partition pruning: b(total 2; left 1)
(10 rows)

explain (verbose, costs off) select * from f_001 where b!=4;
--?.*
HINT:  Do analyze for them in order to generate optimized plan.
--?.*
--?.*
 Row Adapter
   Output: a, b, c
   ->  Vector Streaming (type: GATHER)
         Output: a, b, c
         ->  Partitioned Vector Foreign Scan on public.f_001
               Output: a, b, c
               Filter: (f_001.b <> 4)
               Server Type: hdfs
               Orc File: /@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.dfs_001
               Partition pruning: b(total 2; left 2)
(10 rows)

explain (verbose, costs off) select * from f_001 where b=2 and c=1;
--?.*
--?.*
--?.*
--?.*
 Row Adapter
   Output: a, b, c
   ->  Vector Streaming (type: GATHER)
         Output: a, b, c
         ->  Partitioned Vector Foreign Scan on public.f_001
               Output: a, b, c
               Filter: ((f_001.b = 2) AND (f_001.c = 1))
               Server Type: hdfs
               Orc File: /@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.dfs_001
               Partition pruning: b(total 2; left 1), c(total 1; left 0)
(10 rows)

explain (verbose, costs off) select * from f_001 where b!=4 and c=1;
--?.*
HINT:  Do analyze for them in order to generate optimized plan.
--?.*
--?.*
 Row Adapter
   Output: a, b, c
   ->  Vector Streaming (type: GATHER)
         Output: a, b, c
         ->  Partitioned Vector Foreign Scan on public.f_001
               Output: a, b, c
               Filter: ((f_001.b <> 4) AND (f_001.c = 1))
               Server Type: hdfs
               Orc File: /@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.dfs_001
               Partition pruning: b(total 2; left 2), c(total 2; left 0)
(10 rows)

select (a+b),b::bigint as cc from f_001 union all select (b-a),a::bigint as dd from f_001 order by 1,2;
 ?column? | cc 
----------+----
        1 |  1
        2 |  1
        3 |  2
        4 |  3
(4 rows)

drop table dfs_001;
drop foreign table f_001;
--encoding check
create table hanzi1(a int, b text) tablespace hdfs_ts;
set cstore_insert_mode='main';
insert into hanzi1 values(1, '中国\0\x\5\45');
create foreign table hanzi2( a int, b text)
server hdfs_server
OPTIONS (format 'orc', foldername '/@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.hanzi1', encoding 'GBK')
distribute by roundrobin;
select * from hanzi2;
 a |       b        
---+----------------
 1 | 涓浗\0\x\5\45
(1 row)

create foreign table hanzi3( a int, b text)
server hdfs_server
OPTIONS (format 'orc', foldername '/@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.hanzi1', encoding 'GBK', checkencoding 'high')
distribute by roundrobin;
select * from hanzi3;
 a |       b        
---+----------------
 1 | 涓浗\0\x\5\45
(1 row)

drop table hanzi1;
drop foreign table hanzi2;
drop foreign table hanzi3;
set cstore_insert_mode=main;
drop table if exists associate_benefit_expense cascade;
NOTICE:  table "associate_benefit_expense" does not exist, skipping
create table associate_benefit_expense
(
    period_end_dt date not null ,
    associate_expns_type_cd varchar(50) not null ,
    benefit_hours_qty decimal(38,11)
) tablespace hdfs_ts distribute by hash(period_end_dt,benefit_hours_qty)
partition by values (associate_expns_type_cd );
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1973-01-01', 'B', NULL);
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1976-01-01', 'C', 2.0 );
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1979-01-01', 'D', 3.0 );
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1982-01-01', 'E', 4.0 );
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1985-01-01', 'F', 5.0 );
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1988-01-01', 'F', NULL);
INSERT INTO ASSOCIATE_BENEFIT_EXPENSE VALUES (DATE '1983-01-03', 'I', 4.0 );
SELECT abe.associate_expns_type_cd,abe.benefit_hours_qty
FROM associate_benefit_expense abe
WHERE abe.benefit_hours_qty >= 2.0
AND abe.benefit_hours_qty <= 6.0
order by 1,2;
 associate_expns_type_cd | benefit_hours_qty 
-------------------------+-------------------
 C                       |     2.00000000000
 D                       |     3.00000000000
 E                       |     4.00000000000
 F                       |     5.00000000000
 I                       |     4.00000000000
(5 rows)

create index i_associate_benefit_expense on associate_benefit_expense(period_end_dt,benefit_hours_qty);
set enable_seqscan=off;
SELECT abe.associate_expns_type_cd,abe.benefit_hours_qty
FROM associate_benefit_expense abe
WHERE abe.benefit_hours_qty >= 2.0
AND abe.benefit_hours_qty <= 6.0
order by 1,2;
 associate_expns_type_cd | benefit_hours_qty 
-------------------------+-------------------
 C                       |     2.00000000000
 D                       |     3.00000000000
 E                       |     4.00000000000
 F                       |     5.00000000000
 I                       |     4.00000000000
(5 rows)

drop table if exists associate_benefit_expense cascade;
--enable_hadoop_env = on
set enable_hadoop_env = on;
set max_query_retry_times = 0;
drop table if exists row_001;
NOTICE:  table "row_001" does not exist, skipping
create table row_001( a int, b int);
ERROR:  It is unsupported to create row/cstore non-temporary/non-unlogged table in hadoop enviroment.
drop table if exists col_001;
NOTICE:  table "col_001" does not exist, skipping
create table col_001( a int, b int);
ERROR:  It is unsupported to create row/cstore non-temporary/non-unlogged table in hadoop enviroment.
drop table if exists row_001;
NOTICE:  table "row_001" does not exist, skipping
create temp table row_001( a int, b int);
drop table if exists col_001;
NOTICE:  table "col_001" does not exist, skipping
create temp table col_001( a int, b int);
drop table if exists row_001;
create unlogged table row_001( a int, b int);
drop table if exists col_001;
create unlogged table col_001( a int, b int);
create index col_001_index on col_001(a) tablespace hdfs_ts;
ERROR:  It is not supported to create index on DFS tablespace.
drop table if exists col_001;
drop table if exists row_001;
set enable_hadoop_env = off;
drop table web_returns_less;
ERROR:  table "web_returns_less" does not exist
drop table if exists t1;
NOTICE:  table "t1" does not exist, skipping
create table web_returns_less
(
        
        wr_fee                  decimal(39,29) ,
        wr_refunded_cash        decimal(19,18) 
 ) tablespace hdfs_ts distribute by hash (wr_refunded_cash,wr_fee);
create table t1 as select * from web_returns_less;
analyze web_returns_less;
alter table web_returns_less     add column d_current_week1 numeric(18,2);
analyze web_returns_less;
alter table web_returns_less     drop  column d_current_week1 ;
analyze web_returns_less;
explain (verbose, costs off) insert into  web_returns_less    select t1.wr_fee, t1.wr_refunded_cash from t1;
--?.*
--?.*
                                      QUERY PLAN                                       
---------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         ->  Vector Insert on public.web_returns_less
               ->  Vector Adapter
                     Output: t1.wr_fee, t1.wr_refunded_cash, (NULL::integer)
                     ->  Streaming(type: REDISTRIBUTE)
                           Output: t1.wr_fee, t1.wr_refunded_cash, (NULL::integer)
                           Distribute Key: t1.wr_refunded_cash, t1.wr_fee
                           ->  Seq Scan on public.t1
                                 Output: t1.wr_fee, t1.wr_refunded_cash, NULL::integer
                                 Distribute Key: t1.wr_fee
(11 rows)

insert into  web_returns_less    select t1.wr_fee, t1.wr_refunded_cash from t1;
drop table web_returns_less;
drop table if exists t1;
create table empty_part_table(a int) tablespace hdfs_ts_part partition by values(a);
set cstore_insert_mode='main';
insert into empty_part_table values(1);
analyze empty_part_table;
set enable_global_stats=off;
analyze empty_part_table;
drop table empty_part_table;
reset cstore_insert_mode;
create schema testschema;
set current_schema=testschema;
set cstore_insert_mode = main;
create table household_demographics_h
(
hd_demo_sk                integer               not null,
hd_income_band_sk         integer                       ,
hd_dep_count              integer
)
with (compression='no',orientation='orc',version=0.12)
tablespace hdfs_ts_part;
insert into household_demographics_h values (5, 6, 7);
create table call_center_h
(
cc_call_center_id         integer               not null,
cc_mkt_id                 integer
)with (compression='no',orientation='orc',version=0.12) 
tablespace hdfs_ts_part
distribute by hash (cc_call_center_id);
insert into call_center_h values (1, 2);
create foreign table household_demographics_f
(
hd_demo_sk                integer               not null,
hd_income_band_sk         integer                       ,
hd_dep_count              integer
)
server hdfs_server
OPTIONS (format 'orc', foldername '/@hdfsstoreplus@/hdfs_ts_part/tablespace_secondary/regression/dts2017052409444.household_demographics_h')
distribute by roundrobin;
create foreign table call_center_f
(
cc_call_center_id         integer               not null,
cc_mkt_id                 integer
)
server hdfs_server
OPTIONS (format 'orc', foldername '/@hdfsstoreplus@/hdfs_ts_part/tablespace_secondary/regression/dts2017052409444.call_center_h')
distribute by roundrobin;
CREATE OR REPLACE PROCEDURE test( id OUT numeric) 
AS
BEGIN 
SELECT count(hd_dep_count) FROM household_demographics_f
GROUP BY hd_income_band_sk >= SOME (SELECT cc_mkt_id FROM call_center_f) limit 1
INTO id;
END;
/
call test(1);
 id 
----
  1
(1 row)

drop foreign table household_demographics_f;
drop foreign table call_center_f;
drop table household_demographics_h cascade;
drop table call_center_h cascade;
drop procedure test;
drop schema testschema cascade;
set current_schema = dfs;
create schema testschema2;
set current_schema=testschema2;
set cstore_insert_mode = main;
create table item_inventory_hdfs
(
location_id number(15,0),
item_inv_dt date,
item_id number(38,5)
)
with (compression='no',orientation='orc',version=0.12)
tablespace hdfs_ts_part
distribute by hash(location_id);
insert into item_inventory_hdfs values ( 8, date '1970-01-01',  20.12);
insert into item_inventory_hdfs values ( 1, date '1973-01-01',  1.3);
insert into item_inventory_hdfs values ( 2, date '1976-01-01',  2.23);
insert into item_inventory_hdfs values ( 3, date '1979-01-01',  3.33);
insert into item_inventory_hdfs values ( 4, date '1982-01-01',  4.98);
insert into item_inventory_hdfs values ( 5, date '1985-01-01',  5.01);
insert into item_inventory_hdfs values(null);
create foreign table item_inventory
(
location_id number(15,0),
item_inv_dt date,
item_id number(38,5)
)
SERVER hdfs_server
OPTIONS
(
FORMAT 'orc',
FOLDERNAME '/@hdfsstoreplus@/hdfs_ts_part/tablespace_secondary/regression/dts2017072002456.item_inventory_hdfs'
)
DISTRIBUTE BY roundrobin;
-- test NIL distribute key
select tt.location_id, sum(cc)
from (select location_id b, count(*) as cc
          from item_inventory
         group by b
        union all
        select location_id b, item_id::bigint as cc
          from item_inventory) s1,
       item_inventory as tt
 where s1.b = tt.location_id
 group by tt.location_id
 order by 1, 2;
 location_id | sum 
-------------+-----
           1 |   2
           2 |   3
           3 |   4
           4 |   6
           5 |   6
           8 |  21
(6 rows)

drop schema testschema2 cascade;
NOTICE:  drop cascades to 2 other objects
DETAIL:  drop cascades to table item_inventory_hdfs
drop cascades to foreign table item_inventory
set current_schema = dfs;
drop tablespace hdfs_ts_part;
\! rm -rf @abs_srcdir@/tmp_check/hdfs_ts_part
\! rm -rf @abs_srcdir@/tmp_check/hdfs_ts_vacuum_test
drop tablespace if exists hdfs_ts_vacuum_test;
NOTICE:  Tablespace "hdfs_ts_vacuum_test" does not exist, skipping.
create tablespace hdfs_ts_vacuum_test location '@abs_srcdir@/tmp_check//hdfs_ts_vacuum_test///' with(filesystem='hdfs', address='@hdfshostname@:@hdfsport@', cfgpath='@hdfscfgpath@//',storepath='/@hdfsstoreplus@///hdfs_ts_vacuum_test//');
create table test_rollback_vacuum(a int,b int) tablespace hdfs_ts_vacuum_test distribute by hash(a);
START TRANSACTION;
set cstore_insert_mode to main;
insert into test_rollback_vacuum values(1,2);
ROLLBACK;
vacuum full test_rollback_vacuum;
create table test_rollback_vacuum_part(a int,b int) tablespace hdfs_ts_vacuum_test distribute by hash(a)
PARTITION BY VALUES (b);
START TRANSACTION;
set cstore_insert_mode to main;
insert into test_rollback_vacuum_part values(1,2);
insert into test_rollback_vacuum_part values(1,0);
insert into test_rollback_vacuum_part values(1,1);
ROLLBACK;
vacuum full test_rollback_vacuum_part;
drop table test_rollback_vacuum;
drop table test_rollback_vacuum_part;
drop tablespace if exists hdfs_ts_vacuum_test;
\! rm -rf @abs_srcdir@/tmp_check/hdfs_ts_vacuum_test
create table multiDropColumn1(c1 int, c2 int, c3 int, c4 int, c5 int, c6 int, c7 int, c8 int, c9 int, c10 int, c11 int, c12 int, c13 int, c14 int, c15 int, c16 int, c17 int, c18 int) tablespace hdfs_ts;
set cstore_insert_mode='main';
insert into multiDropColumn1 values(1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18);
alter table multiDropColumn1 drop column c8;
alter table multiDropColumn1 drop column c9;
alter table multiDropColumn1 drop column c10;
alter table multiDropColumn1 drop column c11;
alter table multiDropColumn1 drop column c12;
alter table multiDropColumn1 drop column c13;
alter table multiDropColumn1 drop column c14;
alter table multiDropColumn1 drop column c15;
alter table multiDropColumn1 drop column c16;
alter table multiDropColumn1 drop column c17;
insert into multiDropColumn1 values(11,12,13,14,15,16,17,18);
select * from multiDropColumn1 order by 1;
 c1 | c2 | c3 | c4 | c5 | c6 | c7 | c18 
----+----+----+----+----+----+----+-----
  1 |  2 |  3 |  4 |  5 |  6 |  7 |  18
 11 | 12 | 13 | 14 | 15 | 16 | 17 |  18
(2 rows)

drop table multiDropColumn1;
reset cstore_insert_mode;
-- DTS exception
create table exception_test1(a int, b varchar) tablespace hdfs_ts;
set cstore_insert_mode='main';
create table exception_row1(a int, b varchar);
insert into exception_row1 values(1,'123');
insert into exception_row1 values(1,'345');
insert into exception_row1 values(1,'456');
insert into exception_test1 select * from exception_row1;
CREATE OR REPLACE FUNCTION get_bir(p_a character varying)
RETURNS numeric
LANGUAGE plpgsql
IMMUTABLE
AS $$ DECLARE  i_date date;
begin
select 1/0;
exception
when others then
return 1;
end $$;
select get_bir(b) from exception_test1;
 get_bir 
---------
       1
       1
       1
(3 rows)

drop function get_bir;
drop table exception_row1;
drop table exception_test1;
create table hdfs_pro_exp_setup_001(
c_smallint smallint not null,
c_double_precision double precision,
c_time_without_time_zone time without time zone null,
c_time_with_time_zone time with time zone,
c_integer integer default 23423,
c_bigint bigint default 923423432,
c_decimal decimal(19) default 923423423,
c_real real,
c_numeric numeric(18,12) null,
c_varchar varchar(19),
c_char char(57) null,
c_timestamp_with_timezone timestamp with time zone,
c_char2 char default '0',
c_text text null,
c_varchar2 varchar2(20),
c_timestamp_without_timezone timestamp without time zone,
c_date date,
c_varchar22 varchar2(11621),
c_numeric2 numeric null,
c_bpchar bpchar(10) not null default 'zzzzzzzzzz',
c_tinyint tinyint not null default 127,
c_interval interval,
c_smalldatetime smalldatetime,
c_tsvector text)
with(orientation='orc',version = 0.12)
tablespace hdfs_ts
distribute by hash(c_smallint,c_integer,c_char);
create or replace procedure hdfs_pro_exp_004(out count_num int) as
begin
  delete from hdfs_pro_exp_setup_001;
  insert into hdfs_pro_exp_setup_001 values(3000,3321.123,'2003-10-10 00:00:00','09:52:04.954925+08',default,default,default,NULL,32.125,'NULL','NULL','2015-10-    10',default,'fafdsafasf','fsdfsassss','2012-2-28','2012-2-28','2012-2-28',32.1545,default,default,'9 days','2012-6-6','fdsfsa ffff rrrr bfgf@fd');
  update hdfs_pro_exp_setup_001 set c_tinyint=null where c_smallint = 3000;
exception
  when INVALID_COLUMN_REFERENCE then
    select count(*) from hdfs_pro_exp_setup_001 into count_num where c_smallint=3;
    return count_num;
  when NOT_NULL_VIOLATION then
    raise WARNING 'column "c_tinyint" violates not-null constraint';
    delete from hdfs_pro_exp_setup_001;
    insert into hdfs_pro_exp_setup_001 values(3000,3321.123,'2003-10-10 00:00:00','09:52:04.954925+08',default,default,default,NULL,32.125,'NULL','NULL','2015-1    0-10',default,'fafdsafasf','fsdfsassss','2012-2-28','2012-2-28','2012-2-28',32.1545,default,default,'9 days','2012-6-6','fdsfsa ffff rrrr bfgf@fd');
    update hdfs_pro_exp_setup_001 set c_tinyint=88 where c_smallint = 3000;
    select count(*) from hdfs_pro_exp_setup_001 into count_num where c_tinyint=88;
end;
/
create or replace procedure hdfs_proexp_call_proexp_7() as
declare
count_num int;
begin
  insert into hdfs_pro_exp_setup_001 values(3000,3321.123,'2003-10-10 00:00:00','09:52:04.954925+08',default,default,default,NULL,32.125,'NULL','NULL','2015-10-10',default,'fafdsafasf',    'fsdfsassss','2012-2-28','2012-2-28','2012-2-28',32.1545,default,default,'9 days','2012-6-6','fdsfsa ffff rrrr bfgf@fd');
  update hdfs_pro_exp_setup_001 set c_tinyint=null where c_smallint = 3000;
  hdfs_pro_exp_004(count_num);
exception
  when OTHERS then
    raise EXCEPTION 'Error Occurs!';
end;
/
call hdfs_proexp_call_proexp_7();
WARNING:  exception error message is: null value in column "c_tinyint" violates not-null constraint
CONTEXT:  PL/pgSQL function hdfs_proexp_call_proexp_7() line 6 during exception cleanup
There are  records in table hdfs_pro_exp_setup_001.
ERROR:  Error Occurs!
drop procedure hdfs_proexp_call_proexp_7;
drop procedure hdfs_pro_exp_004;
drop table hdfs_pro_exp_setup_001;
reset cstore_insert_mode;
create table compact1(a int, b int) tablespace hdfs_ts;
set cstore_insert_mode='main';
insert into compact1 values(1,1);
insert into compact1 values(2,2);
vacuum full compact compact1;
insert into compact1 values(3,3);
delete from compact1 where a=2;
vacuum full compact compact1;
insert into compact1 values(4,4);
select * from compact1 order by 1,2;
 a | b 
---+---
 1 | 1
 3 | 3
 4 | 4
(3 rows)

drop table compact1;
reset cstore_insert_mode;
-- llt add
set cstore_insert_mode=main;
set enable_seqscan=off;
create table index_llt_1(a int, b text,c int) with(orientation=column);
create index on index_llt_1 using btree(a,b);
insert into index_llt_1 values(1,'123', 2);
select a,b from index_llt_1;
 a |  b  
---+-----
 1 | 123
(1 row)

create table col_replica_1(a int) with(orientation=column) distribute by replication;
insert into col_replica_1 values(1);
set enable_nestloop=true;
set enable_mergejoin=false;
set enable_hashjoin=false;
analyze col_replica_1;
explain (costs off) 
select count(*) from col_replica_1,index_llt_1 where index_llt_1.a = col_replica_1.a;
                                          QUERY PLAN                                           
-----------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Aggregate
         ->  Vector Streaming (type: GATHER)
               ->  Vector Aggregate
                     ->  Vector Nest Loop
                           ->  CStore Scan on col_replica_1
                                 Filter: (Hash By a)
                           ->  CStore Index Only Scan using index_llt_1_a_b_idx on index_llt_1
                                 Index Cond: (a = $0)
(9 rows)

select count(*) from col_replica_1,index_llt_1 where index_llt_1.a = col_replica_1.a;
 count 
-------
     1
(1 row)

explain (costs off) 
select index_llt_1.c from col_replica_1,index_llt_1 where index_llt_1.a = col_replica_1.a;
                                  QUERY PLAN                                  
------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         ->  Vector Nest Loop
               ->  CStore Scan on col_replica_1
                     Filter: (Hash By a)
               ->  CStore Index Scan using index_llt_1_a_b_idx on index_llt_1
                     Index Cond: (a = $0)
(7 rows)

select index_llt_1.c from col_replica_1,index_llt_1 where index_llt_1.a = col_replica_1.a;
 c 
---
 2
(1 row)

drop table index_llt_1;
drop table col_replica_1;
reset enable_nestloop;
reset enable_mergejoin;
reset enable_hashjoin;
create table index_llt_2(a int, b int) with(orientation=column) partition by range(b)(partition p1 values less than(5), partition p2 values less than(10));
create index on index_llt_2 using btree(b) local;
insert into index_llt_2 values(1,1);
insert into index_llt_2 values(1,7);
insert into index_llt_2 values(1,1);
insert into index_llt_2 values(1,7);
explain (costs off) select * from index_llt_2 where b = (select a from index_llt_2 limit 1);
                                        QUERY PLAN                                        
------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         InitPlan 1 (returns $1)
           ->  Row Adapter
                 ->  Vector Streaming(type: BROADCAST)
                       ->  Vector Limit
                             ->  Vector Streaming(type: BROADCAST)
                                   ->  Vector Limit
                                         ->  Vector Partition Iterator
                                               Iterations: 2
                                               ->  Partitioned CStore Scan on index_llt_2
                                                     Selected Partitions:  1..2
         ->  Vector Partition Iterator
               Iterations: 2
               ->  Partitioned CStore Index Scan using index_llt_2_b_idx on index_llt_2
                     Index Cond: (b = $1)
                     Selected Partitions:  1..2
(17 rows)

select * from index_llt_2 where b = (select a from index_llt_2 limit 1) order by 1,2;
 a | b 
---+---
 1 | 1
 1 | 1
(2 rows)

explain (costs off) select b from index_llt_2 where b = (select a from index_llt_2 limit 1);
                                         QUERY PLAN                                          
---------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         InitPlan 1 (returns $1)
           ->  Row Adapter
                 ->  Vector Streaming(type: BROADCAST)
                       ->  Vector Limit
                             ->  Vector Streaming(type: BROADCAST)
                                   ->  Vector Limit
                                         ->  Vector Partition Iterator
                                               Iterations: 2
                                               ->  Partitioned CStore Scan on index_llt_2
                                                     Selected Partitions:  1..2
         ->  Vector Partition Iterator
               Iterations: 2
               ->  Partitioned CStore Index Only Scan using index_llt_2_b_idx on index_llt_2
                     Index Cond: (b = $1)
                     Selected Partitions:  1..2
(17 rows)

select b from index_llt_2 where b = (select a from index_llt_2 limit 1) order by 1;
 b 
---
 1
 1
(2 rows)

drop table index_llt_2;
create table index_llt_3(a int, b int) with(orientation=column);
create index on index_llt_3 using btree(b);
insert into index_llt_3 values(1,1);
insert into index_llt_3 values(1,7);
insert into index_llt_3 values(1,1);
insert into index_llt_3 values(1,7);
explain (costs off) select * from index_llt_3 where b = (select a from index_llt_3 limit 1);
                               QUERY PLAN                               
------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 ->  Vector Streaming(type: BROADCAST)
                       ->  Vector Limit
                             ->  Vector Streaming(type: BROADCAST)
                                   ->  Vector Limit
                                         ->  CStore Scan on index_llt_3
         ->  CStore Index Scan using index_llt_3_b_idx on index_llt_3
               Index Cond: (b = $0)
(11 rows)

select * from index_llt_3 where b = (select a from index_llt_3 limit 1) order by 1,2;
 a | b 
---+---
 1 | 1
 1 | 1
(2 rows)

explain (costs off) select b from index_llt_3 where b = (select a from index_llt_3 limit 1);
                                QUERY PLAN                                 
---------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 ->  Vector Streaming(type: BROADCAST)
                       ->  Vector Limit
                             ->  Vector Streaming(type: BROADCAST)
                                   ->  Vector Limit
                                         ->  CStore Scan on index_llt_3
         ->  CStore Index Only Scan using index_llt_3_b_idx on index_llt_3
               Index Cond: (b = $0)
(11 rows)

select b from index_llt_3 where b = (select a from index_llt_3 limit 1) order by 1;
 b 
---
 1
 1
(2 rows)

drop table index_llt_3;
create table index_llt_4(a int, b int) tablespace hdfs_ts;
create index on index_llt_4 using btree(b);
insert into index_llt_4 values(1,1);
insert into index_llt_4 values(1,7);
insert into index_llt_4 values(1,1);
insert into index_llt_4 values(1,7);
explain (costs off) select * from index_llt_4 where b = (select a from index_llt_4 limit 1);
                                                   QUERY PLAN                                                   
----------------------------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 ->  Vector Streaming(type: BROADCAST)
                       ->  Vector Limit
                             ->  Vector Streaming(type: BROADCAST)
                                   ->  Vector Limit
                                         ->  Vector Result
                                               ->  Vector Append
                                                     ->  Dfs Scan on index_llt_4
                                                     ->  Vector Adapter
                                                           ->  Seq Scan on pg_delta_dfs_index_llt_4 index_llt_4
         ->  Vector Result
               ->  Vector Append
                     ->  Dfs Index Scan using index_llt_4_b_idx on index_llt_4
                           Index Cond: (b = $0)
                     ->  Vector Adapter
                           ->  Seq Scan on pg_delta_dfs_index_llt_4 index_llt_4
                                 Filter: (b = $0)
(20 rows)

select * from index_llt_4 where b = (select a from index_llt_4 limit 1) order by 1,2;
 a | b 
---+---
 1 | 1
 1 | 1
(2 rows)

explain (costs off) select b from index_llt_4 where b = (select a from index_llt_4 limit 1);
                                                   QUERY PLAN                                                   
----------------------------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         InitPlan 1 (returns $0)
           ->  Row Adapter
                 ->  Vector Streaming(type: BROADCAST)
                       ->  Vector Limit
                             ->  Vector Streaming(type: BROADCAST)
                                   ->  Vector Limit
                                         ->  Vector Result
                                               ->  Vector Append
                                                     ->  Dfs Scan on index_llt_4
                                                     ->  Vector Adapter
                                                           ->  Seq Scan on pg_delta_dfs_index_llt_4 index_llt_4
         ->  Vector Result
               ->  Vector Append
                     ->  Dfs Index Only Scan using index_llt_4_b_idx on index_llt_4
                           Index Cond: (b = $0)
                     ->  Vector Adapter
                           ->  Seq Scan on pg_delta_dfs_index_llt_4 index_llt_4
                                 Filter: (b = $0)
(20 rows)

select b from index_llt_4 where b = (select a from index_llt_4 limit 1) order by 1;
 b 
---
 1
 1
(2 rows)

drop table index_llt_4;
create table index_llt_5(a int, b int) tablespace hdfs_ts;
create table index_llt_5_row(a int, b int);
insert into index_llt_5_row values(10,1),(10,2);
insert into index_llt_5 select * from index_llt_5_row;
insert into index_llt_5 values(8,3);
create index on index_llt_5 using btree(b);
explain (costs off) select * from index_llt_5 where a < 10 and b < 3 order by 1,2;
                                      QUERY PLAN                                      
--------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         Merge Sort Key: dfs.index_llt_5.a, dfs.index_llt_5.b
         ->  Vector Sort
               Sort Key: dfs.index_llt_5.a, dfs.index_llt_5.b
               ->  Vector Result
                     ->  Vector Append
                           ->  Dfs Index Scan using index_llt_5_b_idx on index_llt_5
                                 Index Cond: (b < 3)
                                 Pushdown Predicate Filter: (a < 10)
                           ->  Vector Adapter
                                 ->  Seq Scan on pg_delta_dfs_index_llt_5 index_llt_5
                                       Filter: ((a < 10) AND (b < 3))
(13 rows)

select * from index_llt_5 where a < 10 and b < 3 order by 1,2;
 a | b 
---+---
(0 rows)

drop table index_llt_5_row;
drop table index_llt_5;
reset enable_seqscan;
create table bf_llt_1(a int, b int) tablespace hdfs_ts partition by values(b);
insert into bf_llt_1 values(1, generate_series(1,100));
create table col_replication_1(a int, b int) with(orientation=column) distribute by replication;
insert into col_replication_1 values(1,1);
analyze col_replication_1;
set enable_nestloop=off;
set enable_mergejoin=off;
analyze col_replication_1;
analyze bf_llt_1;
explain (costs off) select * from bf_llt_1,col_replication_1 where bf_llt_1.b = col_replication_1.b;
                                QUERY PLAN                                
--------------------------------------------------------------------------
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         ->  Vector Sonic Hash Join
               Hash Cond: (dfs.bf_llt_1.b = col_replication_1.b)
               Generate Bloom Filter On Expr: col_replication_1.b
               Generate Bloom Filter On Index: 0
               ->  Vector Append
                     ->  Partitioned Dfs Scan on bf_llt_1
                           Filter By Bloom Filter On Expr: dfs.bf_llt_1.b
                           Filter By Bloom Filter On Index: 0
                     ->  Vector Adapter
                           ->  Seq Scan on pg_delta_dfs_bf_llt_1 bf_llt_1
               ->  CStore Scan on col_replication_1
(13 rows)

select * from bf_llt_1,col_replication_1 where bf_llt_1.b = col_replication_1.b order by 1,2;
 a | b | a | b 
---+---+---+---
 1 | 1 | 1 | 1
(1 row)

drop table bf_llt_1;
drop table col_replication_1;
reset enable_nestloop;
reset enable_mergejoin;
--weici xiatui
create table timestamp2(a int, b timestamp) tablespace hdfs_ts;
set cstore_insert_mode='main';
set time zone 'PRC';
insert into timestamp2 values(1, '20150327');
insert into timestamp2 values(1, '20150327 00:00:00');
insert into timestamp2 values(1, '20150327 15:00:00');
insert into timestamp2 values(1, '20150327 00:00:00+09');
select * from timestamp2 where b='20150327';
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327'::timestamptz;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00'::timestamptz;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+08'::timestamptz;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+09'::timestamptz;
 a | b 
---+---
(0 rows)

select * from timestamp2 where b='20150327'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+08'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+09'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+08'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+09'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

set enable_hdfs_predicate_pushdown=off;
select * from timestamp2 where b='20150327';
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327'::timestamptz;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00'::timestamptz;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+08'::timestamptz;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+09'::timestamptz;
 a | b 
---+---
(0 rows)

select * from timestamp2 where b='20150327'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+08'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+09'::date;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+08'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

select * from timestamp2 where b='20150327 00:00:00+09'::smalldatetime;
 a |            b             
---+--------------------------
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
 1 | Fri Mar 27 00:00:00 2015
(3 rows)

reset enable_hdfs_predicate_pushdown;
drop table timestamp2;
reset time zone;
reset cstore_insert_mode;
create table test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb (a int, b int) tablespace hdfs_ts;
NOTICE:  identifier "test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb" will be truncated to "test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb"
insert into test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb values(1,generate_series(1,100));
analyze test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb;
analyze test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb;
analyze;
analyze;
drop table test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb;
NOTICE:  identifier "test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb" will be truncated to "test_hdfs_delta_bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb"
-- llt add
reset current_schema;
create table llt_addition_1(a int, b int) tablespace hdfs_ts;
set cstore_insert_mode=main;
set schedule_splits_threshold = 1;
insert into llt_addition_1 values(1,1);
insert into llt_addition_1 values(2,2);
insert into llt_addition_1 values(3,3);
create foreign table llt_addition_1f( a int, b int)
server hdfs_server
OPTIONS (format 'orc', foldername '/@hdfsstoreplus@/dfs_init_004/tablespace_secondary/regression/public.llt_addition_1')
distribute by roundrobin;
select * from llt_addition_1f order by 1;
 a | b 
---+---
 1 | 1
 2 | 2
 3 | 3
(3 rows)

explain (costs off) select * from llt_addition_1f order by 1;
--?.*
--?.*
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         Merge Sort Key: a
         ->  Vector Sort
               Sort Key: a
               ->  Vector Foreign Scan on llt_addition_1f
                     Server Type: hdfs
--?.*
(8 rows)

explain analyze select * from llt_addition_1f order by 1;
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
--?.*
(12 rows)

set query_dop=0;
explain (costs off) select * from llt_addition_1f;
--?.*
--?.*
 Row Adapter
   ->  Vector Streaming (type: GATHER)
         ->  Vector Streaming(type: LOCAL GATHER dop: 1/2)
               ->  Vector Foreign Scan on llt_addition_1f
                     Server Type: hdfs
--?.*
(6 rows)

reset schedule_splits_threshold;
reset cstore_insert_mode;
reset query_dop;
drop foreign table llt_addition_1f;
drop table llt_addition_1;
-- index only scan llt
create table llt_addition_2(a int, b int, c int) tablespace hdfs_ts;
create table row_addition_2(a int, b int, c int);
insert into row_addition_2 values(1, 5, generate_series(1,10001));
insert into row_addition_2 select * from row_addition_2;
insert into row_addition_2 select * from row_addition_2;
insert into row_addition_2 select * from row_addition_2;
set cstore_insert_mode=main;
insert into llt_addition_2 select * from row_addition_2;
analyze llt_addition_2;
set enable_seqscan=off;
create index llt_addition_idx1 on llt_addition_2(b);
explain (costs off) select count(*) from llt_addition_2 where b = 5;
                                             QUERY PLAN                                              
-----------------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Aggregate
         ->  Vector Streaming (type: GATHER)
               ->  Vector Aggregate
                     ->  Vector Append
                           ->  Dfs Index Only Scan using llt_addition_idx1 on llt_addition_2
                                 Index Cond: (b = 5)
                           ->  Vector Streaming(type: LOCAL GATHER dop: 1/2)
                                 ->  Vector Adapter
                                       ->  Seq Scan on pg_delta_public_llt_addition_2 llt_addition_2
                                             Filter: (b = 5)
(11 rows)

select count(*) from llt_addition_2 where b = 5;
 count 
-------
 80008
(1 row)

drop index llt_addition_idx1;
create index llt_addition_idx2 on llt_addition_2 using btree(b);
explain (costs off) select count(*) from llt_addition_2 where b = 5;
                                             QUERY PLAN                                              
-----------------------------------------------------------------------------------------------------
 Row Adapter
   ->  Vector Aggregate
         ->  Vector Streaming (type: GATHER)
               ->  Vector Aggregate
                     ->  Vector Append
                           ->  Dfs Index Only Scan using llt_addition_idx2 on llt_addition_2
                                 Index Cond: (b = 5)
                           ->  Vector Streaming(type: LOCAL GATHER dop: 1/2)
                                 ->  Vector Adapter
                                       ->  Seq Scan on pg_delta_public_llt_addition_2 llt_addition_2
                                             Filter: (b = 5)
(11 rows)

select count(*) from llt_addition_2 where b = 5;
 count 
-------
 80008
(1 row)

drop index llt_addition_idx2;
drop table row_addition_2;
drop table llt_addition_2;
reset enable_seqscan;
reset cstore_insert_mode;
drop server hdfs_server cascade;
select * from gs_fault_inject(2,2,0,0,0,0);
 gs_fault_inject 
-----------------
               0
(1 row)

